{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdzgfuy7uyjQ",
        "outputId": "b090e388-aae5-4aef-ecc7-5d4a9742feb8"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVWUIL4W2bfA",
        "outputId": "e782c456-44bb-48bc-84a7-aa8bbb8cad7d"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "od.download('https://www.kaggle.com/datasets/vaibhavkumar11/hindi-english-parallel-corpus')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTGo7VpH25l6",
        "outputId": "321ec0d1-5d69-4d8c-bfd5-1128371e145a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('hindi-english-parallel-corpus/hindi_english_parallel.csv')\n",
        "df = df.dropna()\n",
        "df=df.reset_index()\n",
        "MAXLEN=10\n",
        "MINLEN = 0\n",
        "hindi=[]\n",
        "english=[]\n",
        "\n",
        "for i in tqdm(range(len(df['hindi']))):\n",
        "    if MINLEN< len(str(df['hindi'][i]).split()) < MAXLEN and  MINLEN< len(str(df['english'][i]).split()) < MAXLEN :\n",
        "        hindi.append(df['hindi'][i])\n",
        "        english.append(df['english'][i])\n",
        "del df\n",
        "def clean_sentence(sentence):\n",
        "    sentence = str(sentence)\n",
        "    sentence = sentence.strip()\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub('[' + string.punctuation + ']', '', sentence)\n",
        "    return sentence\n",
        "hindi = list(map(clean_sentence,hindi))\n",
        "english = list(map(clean_sentence,english))\n",
        "cleaned_data = {\n",
        "    'english':english,\n",
        "    'hindi':hindi\n",
        "}\n",
        "df  = pd.DataFrame.from_dict(cleaned_data)\n",
        "if not os.path.exists('data'):\n",
        "    os.mkdir('data')\n",
        "df.to_csv('data/cleaned_english_hindi_corpus.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "173EaF7vnPku"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kqaiV6mnPkv"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/cleaned_english_hindi_corpus.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrwD1EO5nPkv",
        "outputId": "f814f36f-2e5f-4d5e-d2f3-1d6075fc963c"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4Qy1fzKnPkw",
        "outputId": "047b9c1c-aa4c-4f80-f8be-89f9371ceddc"
      },
      "outputs": [],
      "source": [
        "SOS_TOKEN =  0\n",
        "EOS_TOKEN = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self,name) -> None:\n",
        "        self.name = name\n",
        "        self.index2word = {0:SOS_TOKEN,1:EOS_TOKEN}\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.n_words = 2\n",
        "    def addWord(self,word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word]=self.n_words\n",
        "            self.word2count[word]=1\n",
        "            self.index2word[self.n_words]=word\n",
        "            self.n_words+=1\n",
        "    def addSentense(self,sentence):\n",
        "        for word in sentence.split():\n",
        "            self.addWord(word)\n",
        "\n",
        "input_lang = Lang('english')\n",
        "output_lang = Lang('hindi')\n",
        "\n",
        "def make_pairs(input_lang,output_lang):\n",
        "    pairs = []\n",
        "    for i in tqdm(range(len(df))):\n",
        "        eng_sent = str(df['english'][i])\n",
        "        hin_sent = str(df['hindi'][i])\n",
        "        input_lang.addSentense(eng_sent)\n",
        "        output_lang.addSentense(hin_sent)\n",
        "        pairs.append([eng_sent,hin_sent])\n",
        "    return input_lang,output_lang,df['english'],df['hindi'],pairs\n",
        "\n",
        "\n",
        "input_lang , output_lang ,eng,hin, pairs = make_pairs(input_lang,output_lang)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "langs={\n",
        "    'input_lang':input_lang,\n",
        "    'output_lang':output_lang\n",
        "}\n",
        "torch.save(langs,'lang.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo7l16-EnPkw",
        "outputId": "abd0cae0-cc65-4758-e340-319955d2d590"
      },
      "outputs": [],
      "source": [
        "input_lang.n_words , output_lang.n_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWhE659PnPkw"
      },
      "outputs": [],
      "source": [
        "MAXLEN=10\n",
        "\n",
        "class LangData(Dataset):\n",
        "    def __init__(self,eng,hin,input_lang,output_lang):\n",
        "        self.eng = eng\n",
        "        self.hin = hin\n",
        "        n = len(self.eng)\n",
        "        # converting into tensors\n",
        "        eng_numpy = np.zeros((n,MAXLEN),dtype=np.int32)\n",
        "        hin_numpy = np.zeros((n,MAXLEN),dtype=np.int32)\n",
        "        for idx in tqdm(range(n)):\n",
        "            inp = eng[idx]\n",
        "            tgt = hin[idx]\n",
        "            inp_id = self.indexFromSentences(input_lang,inp)\n",
        "            tgt_id = self.indexFromSentences(output_lang,tgt)\n",
        "            inp_id.append(EOS_TOKEN)\n",
        "            tgt_id.append(EOS_TOKEN)\n",
        "            eng_numpy[idx,:len(inp_id)]=inp_id\n",
        "            hin_numpy[idx,:len(tgt_id)]=tgt_id\n",
        "        self.eng_tensor = torch.LongTensor(eng_numpy)\n",
        "        self.hin_tensor = torch.LongTensor(hin_numpy)\n",
        "    def indexFromSentences(self,lang , sent):\n",
        "        # try:\n",
        "        return  [lang.word2index[word] for word in str(sent).split()]\n",
        "        # except:\n",
        "        #     print(sent)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.eng_tensor[index],self.hin_tensor[index]\n",
        "    def __len__(self):\n",
        "        return len(self.eng)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc8L0S624awt"
      },
      "outputs": [],
      "source": [
        "del df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ye-qE2SnPkx",
        "outputId": "626faded-9f48-4485-f872-b7a8d7a80d53"
      },
      "outputs": [],
      "source": [
        "batch_size=64\n",
        "taking = 2000\n",
        "langdataset = LangData(eng[:taking],hin[:taking],input_lang,output_lang)\n",
        "print(len(langdataset))\n",
        "print(len(langdataset))\n",
        "dataloader = DataLoader(langdataset,batch_size=batch_size,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlS8zSghnPkx",
        "outputId": "5d33f905-5c42-43f0-b245-7a9eea315e2e"
      },
      "outputs": [],
      "source": [
        "for data in dataloader:\n",
        "    a,b = data\n",
        "    print(a,b)\n",
        "    print(f'shape = [batchsize , maxlength]')\n",
        "    print(f'input_shape {a.shape} , output_shape {b.shape}')\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU9-Q1VrnPkx"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim ,hidden_dim,p=0.5) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.embedding = nn.Embedding(input_dim,hidden_dim)\n",
        "        self.lstm = nn.GRU(hidden_dim,hidden_dim,num_layers=1,batch_first=True)\n",
        "    def forward(self,x):\n",
        "        emb = self.dropout(self.embedding(x))\n",
        "        output,hidden = self.lstm(emb)\n",
        "        return output , hidden\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim) -> None:\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(hidden_dim,hidden_dim)\n",
        "        self.l2 = nn.Linear(hidden_dim,hidden_dim)\n",
        "        self.l3 = nn.Linear(hidden_dim,1)\n",
        "    def forward(self,encoder_output , prev_hidden):\n",
        "        score = self.l3(torch.tanh(self.l1(encoder_output)+self.l2(prev_hidden)))\n",
        "        weights = torch.softmax(score,dim=1)\n",
        "        weights = weights.permute(0,2,1)\n",
        "        context = torch.bmm(weights,encoder_output)\n",
        "        return context , weights\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim , hidden_dim , p=0.5) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim,hidden_dim)\n",
        "        self.attention = AttentionLayer(hidden_dim)\n",
        "        self.lstm = nn.GRU(2*hidden_dim,hidden_dim,batch_first = True)\n",
        "        self.out = nn.Linear(hidden_dim,output_dim)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "    def forward(self,encoder_output , encoder_hidden,target=None,teacher_forcing_ratio=0.5):\n",
        "        batch_size = encoder_output.shape[0]\n",
        "        max_len = target.shape[1] if target is not None else MAXLEN\n",
        "        decoder_input = torch.zeros(batch_size,1,dtype=torch.long).to(device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        outputs = []\n",
        "        use_teacher_forcing = True if torch.rand(1).item() < teacher_forcing_ratio and target is not None else False\n",
        "\n",
        "        if use_teacher_forcing:\n",
        "            for i in range(max_len):\n",
        "                output, decoder_hidden, weights = self.model(decoder_input, decoder_hidden, encoder_output)\n",
        "                outputs.append(output)\n",
        "                decoder_input = target[:, i].unsqueeze(1)  # Use target input at current time step\n",
        "        else:\n",
        "            for i in range(max_len):\n",
        "                output, decoder_hidden, weights = self.model(decoder_input, decoder_hidden, encoder_output)\n",
        "                outputs.append(output)\n",
        "                decoder_input = output.argmax(-1)  # Use model's output as input at current time step\n",
        "\n",
        "        outputs = torch.cat(outputs,dim=1)\n",
        "        outputs = F.log_softmax(outputs,dim=2)\n",
        "        return outputs,decoder_hidden\n",
        "    def model(self,input,hidden,encoder_outputs):\n",
        "        embedding = self.dropout(self.embedding(input))\n",
        "        attention_hidden = hidden.permute(1,0,2)\n",
        "        context,weights = self.attention(encoder_outputs,attention_hidden)\n",
        "        input_gru = torch.cat((embedding,context),dim=2)\n",
        "        output , hidden = self.lstm(input_gru,hidden)\n",
        "        prediction = self.out(output)\n",
        "        return prediction , hidden , weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCg09XYenPkx"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(input_lang.n_words,hidden_dim=256).to(device)\n",
        "decoder = Decoder(output_lang.n_words,256).to(device)\n",
        "enc_optim = torch.optim.Adam(encoder.parameters(),lr=0.001)\n",
        "dec_optim = torch.optim.Adam(decoder.parameters(),lr=0.001)\n",
        "criteriion = nn.NLLLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10Rb9wynnPky"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('checkpoint/checkpoint.pth'):\n",
        "    os.mkdir('checkpoint')\n",
        "    checkpoint = {\n",
        "        'encoder':encoder.state_dict(),\n",
        "        'decoder':decoder.state_dict(),\n",
        "        'encoder_optimizer':enc_optim.state_dict(),\n",
        "        'decoder_optimizer':dec_optim.state_dict()\n",
        "    }\n",
        "else:\n",
        "    checkpoint = torch.load('checkpoint/checkpoint.pth',map_location=torch.device(device))\n",
        "    encoder.load_state_dict(checkpoint['encoder'])\n",
        "    decoder.load_state_dict(checkpoint['decoder'])\n",
        "    enc_optim.load_state_dict(checkpoint['encoder_optimizer'])\n",
        "    dec_optim.load_state_dict(checkpoint['decoder_optimizer'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "1/0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4bfgXmQnPky",
        "outputId": "f8033a5e-c1bd-4346-89c2-ca93b91b7dd0"
      },
      "outputs": [],
      "source": [
        "for epoch in range(30):\n",
        "    e_loss = 0\n",
        "    print(f'Epoch {epoch}')\n",
        "    st = time.time()\n",
        "    for data in dataloader:\n",
        "        input_tensor , target_tensor = data\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        target_tensor = target_tensor.to(device)\n",
        "\n",
        "        encoder_output , encoder_hidden = encoder(input_tensor)\n",
        "        outputs , _ = decoder(encoder_output,encoder_hidden,target_tensor)\n",
        "\n",
        "        loss = criteriion(outputs.view(-1,outputs.size(-1)) ,target_tensor.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        enc_optim.step()\n",
        "        dec_optim.step()\n",
        "\n",
        "        enc_optim.zero_grad()\n",
        "        dec_optim.zero_grad()\n",
        "\n",
        "        e_loss = e_loss+loss.item()\n",
        "    if epoch%1==0:\n",
        "        checkpoint['encoder']=encoder.state_dict()\n",
        "        checkpoint['decoder']=decoder.state_dict()\n",
        "        checkpoint['encoder_optimizer']=enc_optim.state_dict()\n",
        "        checkpoint['decoder_optimizer']=dec_optim.state_dict()\n",
        "        torch.save(checkpoint,'checkpoint/checkpoint.pth')\n",
        "        print('Checkpoint_saved')\n",
        "\n",
        "    print('-'*30)\n",
        "    print(f'loss {e_loss/len(dataloader):.3f}')\n",
        "    print(f'time taken {(time.time()-st)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8Vt_GfqnPky"
      },
      "outputs": [],
      "source": [
        "def indexFromSentence(lang , sent):\n",
        "        return  [lang.word2index[word] for word in str(sent).split()]\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_TOKEN)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "def anuvadkaro(sent):\n",
        "  with torch.no_grad():\n",
        "          inp = tensorFromSentence(input_lang,sent)\n",
        "          # print(f'indexes={inp}')\n",
        "          enc_out , enc_hidden = encoder(inp)\n",
        "          dec_out , dec_hid = decoder(enc_out , enc_hidden)\n",
        "          # print(dec_out.shape)\n",
        "          dec_out = dec_out.argmax(-1)\n",
        "          decoded_ids = dec_out.squeeze()\n",
        "          decoded_words = []\n",
        "          # print(f'decoded_ids={decoded_ids}')\n",
        "          for idx in decoded_ids:\n",
        "              if idx.item() == EOS_TOKEN:\n",
        "                  decoded_words.append('<EOS>')\n",
        "                  break\n",
        "              else:\n",
        "                  decoded_words.append(output_lang.index2word[idx.item()])\n",
        "  return decoded_words[:-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZeU2FzpySB1",
        "outputId": "036784ec-338e-40f7-ec13-fe65e502f519"
      },
      "outputs": [],
      "source": [
        "print(anuvadkaro('highlight duration'))\n",
        "print(anuvadkaro('perform action'))\n",
        "print(anuvadkaro('too many selectable children')) #बहुत अधिक चयनीय शिशु हैं"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
