{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "MAXLEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim ,hidden_dim,p=0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_dim,hidden_dim)\n",
    "        self.lstm = nn.GRU(hidden_dim,hidden_dim,num_layers=1,batch_first=True)\n",
    "    def forward(self,x):\n",
    "        emb = self.dropout(self.embedding(x))\n",
    "        output,hidden = self.lstm(emb)\n",
    "        return output , hidden\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim,1)\n",
    "    def forward(self,encoder_output , prev_hidden):\n",
    "        score = self.l3(torch.tanh(self.l1(encoder_output)+self.l2(prev_hidden)))\n",
    "        weights = torch.softmax(score,dim=1)\n",
    "        weights = weights.permute(0,2,1)\n",
    "        context = torch.bmm(weights,encoder_output)\n",
    "        return context , weights\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim , hidden_dim , p=0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim,hidden_dim)\n",
    "        self.attention = AttentionLayer(hidden_dim)\n",
    "        self.lstm = nn.GRU(2*hidden_dim,hidden_dim,batch_first = True)\n",
    "        self.out = nn.Linear(hidden_dim,output_dim)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "    def forward(self,encoder_output , encoder_hidden,target=None,teacher_forcing_ratio=0.5):\n",
    "        batch_size = encoder_output.shape[0]\n",
    "        max_len = target.shape[1] if target is not None else MAXLEN\n",
    "        decoder_input = torch.zeros(batch_size,1,dtype=torch.long).to(device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        outputs = []\n",
    "        use_teacher_forcing = True if torch.rand(1).item() < teacher_forcing_ratio and target is not None else False\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            for i in range(max_len):\n",
    "                output, decoder_hidden, weights = self.model(decoder_input, decoder_hidden, encoder_output)\n",
    "                outputs.append(output)\n",
    "                decoder_input = target[:, i].unsqueeze(1)  # Use target input at current time step\n",
    "        else:\n",
    "            for i in range(max_len):\n",
    "                output, decoder_hidden, weights = self.model(decoder_input, decoder_hidden, encoder_output)\n",
    "                outputs.append(output)\n",
    "                decoder_input = output.argmax(-1)  # Use model's output as input at current time step\n",
    "\n",
    "        outputs = torch.cat(outputs,dim=1)\n",
    "        outputs = F.log_softmax(outputs,dim=2)\n",
    "        return outputs,decoder_hidden\n",
    "    def model(self,input,hidden,encoder_outputs):\n",
    "        embedding = self.dropout(self.embedding(input))\n",
    "        attention_hidden = hidden.permute(1,0,2)\n",
    "        context,weights = self.attention(encoder_outputs,attention_hidden)\n",
    "        input_gru = torch.cat((embedding,context),dim=2)\n",
    "        output , hidden = self.lstm(input_gru,hidden)\n",
    "        prediction = self.out(output)\n",
    "        return prediction , hidden , weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "class Lang:\n",
    "    def __init__(self,name) -> None:\n",
    "        self.name = name\n",
    "        self.index2word = {0:SOS_TOKEN,1:EOS_TOKEN}\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 2\n",
    "    def addWord(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word]=self.n_words\n",
    "            self.word2count[word]=1\n",
    "            self.index2word[self.n_words]=word\n",
    "            self.n_words+=1\n",
    "    def addSentense(self,sentence):\n",
    "        for word in sentence.split():\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs = torch.load('lang.pth')\n",
    "input_lang = langs['input_lang']\n",
    "output_lang = langs['output_lang']\n",
    "encoder = Encoder(input_lang.n_words,hidden_dim=256).to(device)\n",
    "decoder = Decoder(output_lang.n_words,256).to(device)\n",
    "checkpoint = torch.load('checkpoint/checkpoint.pth',map_location=torch.device(device))\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexFromSentence(lang , sent):\n",
    "        return  [lang.word2index[word] for word in str(sent).split()]\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_TOKEN)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "def anuvadkaro(sent):\n",
    "  with torch.no_grad():\n",
    "          inp = tensorFromSentence(input_lang,sent)\n",
    "          # print(f'indexes={inp}')\n",
    "          enc_out , enc_hidden = encoder(inp)\n",
    "          dec_out , dec_hid = decoder(enc_out , enc_hidden)\n",
    "          # print(dec_out.shape)\n",
    "          dec_out = dec_out.argmax(-1)\n",
    "          decoded_ids = dec_out.squeeze()\n",
    "          decoded_words = []\n",
    "          # print(f'decoded_ids={decoded_ids}')\n",
    "          for idx in decoded_ids:\n",
    "              if idx.item() == EOS_TOKEN:\n",
    "                  decoded_words.append('<EOS>')\n",
    "                  break\n",
    "              else:\n",
    "                  decoded_words.append(output_lang.index2word[idx.item()])\n",
    "  return decoded_words[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['हाइलाइट', 'अवधिः', 'हाइलाइट', 'रकें']\n",
      "['कार्रवाई', 'संपन्न', 'करें']\n",
      "['बहुत', 'अधिक', 'चयनीय', 'शिशु', 'हैं']\n"
     ]
    }
   ],
   "source": [
    "print(anuvadkaro('highlight duration'))\n",
    "print(anuvadkaro('perform action'))\n",
    "print(anuvadkaro('too many selectable children')) #बहुत अधिक चयनीय शिशु हैं"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
